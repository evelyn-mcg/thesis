{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8a5147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eviemcgonigle/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/eviemcgonigle/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1b8570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#for pos tagging\n",
    "\n",
    "#!pip install spacy\n",
    "import spacy\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "en_spacy = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3f6ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import re #regex for cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e6b658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/eviemcgonigle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/eviemcgonigle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eviemcgonigle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/eviemcgonigle/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6383905",
   "metadata": {},
   "source": [
    "# IMPORT DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d0e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_posts(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for json_str in file:\n",
    "        # load lines\n",
    "            result = json.loads(json_str)\n",
    "\n",
    "            # extract fields\n",
    "            data.append({\n",
    "                'selftext': result.get('selftext', None),\n",
    "                'title': result.get('title', None),\n",
    "                'author': result.get('author', None)\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db055317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_comments(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for json_str in file:\n",
    "        # load lines\n",
    "            result = json.loads(json_str)\n",
    "\n",
    "            # extract fields\n",
    "            data.append({\n",
    "                'author': result.get('author', None),\n",
    "                'selftext': result.get('body', None)\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    #add in empty column 'title' to use concat function\n",
    "    df['title'] = np.nan\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cd40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove posts from known bots\n",
    "#and posts where content is removed\n",
    "def remove_unhelpful(data):\n",
    "    df = data.copy()\n",
    "    \n",
    "    #remove bots\n",
    "    bot_authors = [\"AutoModerator\", \"ClickableLinkBot\", \"election_info_bot\", \n",
    "                   \"outline_link_bot\", \"LearnDifferenceBot\"]\n",
    "    df = df[~df['author'].isin(bot_authors)]\n",
    "    \n",
    "    \n",
    "    #remove empty deleted posts/comments\n",
    "    df = df[~(((df['selftext'] == \"[removed]\")|(df['selftext'] == \"[deleted]\")) \n",
    "              & (df['title'].isna() | (df['title'] == \"\") | (df['title'] == \"[deleted by user]\")))]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Random Downsampling\n",
    "# make each df the length of the smallest df\n",
    "# with same number of unique authors\n",
    "def rand_downsampling(d23, d19, r23, r19, rstate):\n",
    "    \n",
    "    #first make each dataset have the same number of unique authors\n",
    "    unique_authors_counts = [\n",
    "        d23['author'].nunique(),\n",
    "        d19['author'].nunique(),\n",
    "        r23['author'].nunique(),\n",
    "        r19['author'].nunique()\n",
    "    ]\n",
    "    min_authors = min(unique_authors_counts)\n",
    "    \n",
    "    #same namber unique authors per corpus\n",
    "    def downsample_author(df, min_authors, rstate):\n",
    "        sampled_authors = df['author'].drop_duplicates().sample(n=min_authors, random_state=rstate)\n",
    "        return df[df['author'].isin(sampled_authors)]\n",
    "    \n",
    "    d23_down = downsample_author(d23, min_authors, rstate)\n",
    "    d19_down = downsample_author(d19, min_authors, rstate)\n",
    "    r23_down = downsample_author(r23, min_authors, rstate)\n",
    "    r19_down = downsample_author(r19, min_authors, rstate)\n",
    "    \n",
    "    lengths = [len(d23_down), len(d19_down), len(r23_down), len(r19_down)]\n",
    "    size = min(lengths)\n",
    "    \n",
    "    final_down_d23 = d23_down.sample(n=size, random_state=rstate).reset_index(drop=True)\n",
    "    final_down_d19 = d19_down.sample(n=size, random_state=rstate).reset_index(drop=True)\n",
    "    final_down_r23 = r23_down.sample(n=size, random_state=rstate).reset_index(drop=True)\n",
    "    final_down_r19 = r19_down.sample(n=size, random_state=rstate).reset_index(drop=True)\n",
    "    \n",
    "    return final_down_d23, final_down_d19, final_down_r23, final_down_r19\n",
    "\n",
    "#combine post and comment data into one df\n",
    "def combo(posts, comments):\n",
    "    return pd.concat([posts, comments], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed326be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTERS\n",
    "\n",
    "def unique_auth(d23, d19, r23, r19):\n",
    "    unique_authors_counts = [\n",
    "        d23['author'].nunique(),\n",
    "        d19['author'].nunique(),\n",
    "        r23['author'].nunique(),\n",
    "        r19['author'].nunique()\n",
    "    ]\n",
    "    return unique_authors_counts\n",
    "\n",
    "def unique_auth_posts(d23, d19, r23, r19, rstate):\n",
    "    unique_authors_counts = [\n",
    "        d23['author'].nunique(),\n",
    "        d19['author'].nunique(),\n",
    "        r23['author'].nunique(),\n",
    "        r19['author'].nunique()\n",
    "    ]\n",
    "    min_authors = min(unique_authors_counts)\n",
    "    \n",
    "    def downsample_by_authors(df, min_authors, rstate):\n",
    "        sampled_authors = df['author'].drop_duplicates().sample(n=min_authors, random_state=rstate)\n",
    "        return df[df['author'].isin(sampled_authors)]\n",
    "    \n",
    "    d23_down = downsample_by_authors(d23, min_authors, rstate)\n",
    "    d19_down = downsample_by_authors(d19, min_authors, rstate)\n",
    "    r23_down = downsample_by_authors(r23, min_authors, rstate)\n",
    "    r19_down = downsample_by_authors(r19, min_authors, rstate)\n",
    "    return d23_down, d19_down, r23_down, r19_down\n",
    "\n",
    "def remove_del(data):\n",
    "    df = data.copy()\n",
    "    df = df[~((df['selftext'] == \"[removed]\") & (df['title'].isna() | (df['title'] == \"\")))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f53fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8766 8766 8766 8766\n"
     ]
    }
   ],
   "source": [
    "#Import Posts\n",
    "\n",
    "p_d23 = remove_unhelpful(import_posts('./data/raw/r_democrats_23posts.jsonl'))\n",
    "p_d19 = remove_unhelpful(import_posts('./data/raw/r_democrats_19posts.jsonl'))\n",
    "p_r23 = remove_unhelpful(import_posts('./data/raw/r_republican_23posts.jsonl'))\n",
    "p_r19 = remove_unhelpful(import_posts('./data/raw/r_republican_19posts.jsonl'))\n",
    "\n",
    "#downsample\n",
    "down_p_d23, down_p_d19, down_p_r23, down_p_r19 = rand_downsampling(p_d23, p_d19, p_r23, p_r19, 3)\n",
    "\n",
    "print(len(down_p_d23), len(down_p_d19), len(down_p_r23), len(down_p_r19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c176af17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20481 20481 20481 20481\n"
     ]
    }
   ],
   "source": [
    "#Import Comments\n",
    "c_d23 = remove_unhelpful(import_comments('./data/raw/r_democrats_23comments.jsonl'))\n",
    "c_d19 = remove_unhelpful(import_comments('./data/raw/r_democrats_19comments.jsonl'))\n",
    "c_r23 = remove_unhelpful(import_comments('./data/raw/r_republican_23comments.jsonl'))\n",
    "c_r19 = remove_unhelpful(import_comments('./data/raw/r_republican_19comments.jsonl'))\n",
    "\n",
    "#downsample\n",
    "down_c_d23, down_c_d19, down_c_r23, down_c_r19 = rand_downsampling(c_d23, \n",
    "                                                                   c_d19, c_r23, c_r19, 10)\n",
    "\n",
    "print(len(down_c_d23), len(down_c_d19), len(down_c_r23), len(down_c_r19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc31a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate posts and comments for each dataset\n",
    "data_d23 = combo(down_p_d23, down_c_d23)\n",
    "data_d19 = combo(down_p_d19, down_c_d19)\n",
    "data_r19 = combo(down_p_r19, down_c_r19)\n",
    "data_r23 = combo(down_p_r23, down_c_r23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45398441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29247 29247 29247 29247\n"
     ]
    }
   ],
   "source": [
    "#check print(len(data_d23), len(data_d19), len(data_r23), len(data_r19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87069834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data\n",
    "data_d23.to_csv('./data/raw/r_democrats_23down_auth.csv', index = False)\n",
    "data_d19.to_csv('./data/raw/r_democrats_19down_auth.csv', index = False)\n",
    "data_r23.to_csv('./data/raw/r_republican_23down_auth.csv', index = False)\n",
    "data_r19.to_csv('./data/raw/r_republican_19down_auth.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f48bd36",
   "metadata": {},
   "source": [
    "# Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43b0716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(df):\n",
    "    #remove deleted/removed from selftext (no added meaning)\n",
    "    df['selftext'] = df['selftext'].replace('[removed]', '', regex=False)\n",
    "    df['selftext'] = df['selftext'].replace('[deleted]', '', regex=False)\n",
    "    \n",
    "    df['selftext'] = df['selftext'].fillna('').astype(str)\n",
    "    df['title'] = df['title'].fillna('').astype(str)\n",
    "\n",
    "    df['text'] = df['selftext'] + \" \" + df['title']\n",
    "    return df[['author', 'text']].copy()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "434e4b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based off evolving divergence paper\n",
    "#removes long posts that are duplicates in text\n",
    "#does NOT remove reprtive Urls - exmained and they are news articles etc\n",
    "def filter_spam(data):\n",
    "    #remove duplicated posts\n",
    "    #df = df[(df['text'].str.len() <= 100) | \n",
    "    #        ((df['text'].str.len() > 100) & \n",
    "    #         ~df.duplicated(subset=['text']))].copy()\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    #remove bots\n",
    "    bot_patterns = [\n",
    "        r\"beep,? boop,? i'?m a bot\", \n",
    "        r\"i am a bot\",  \n",
    "        r\"this action was performed automatically\",  \n",
    "        r\"please contact the moderators\",  \n",
    "        r\"source code for this bot\",  \n",
    "        r\"this response was generated\",\n",
    "        r\"Bot message:\"\n",
    "    ]\n",
    "    bot_regex = \"|\".join(bot_patterns)  # Join patterns into a single regex\n",
    "    df = df[~df['text'].str.contains(bot_regex, case=False, na=False, regex=True)]\n",
    "    \n",
    "#     df.loc[:, 'url'] = df['text'].str.extract(r\"(https://\\S+)\")\n",
    "#     spam_urls = df['url'].value_counts().nlargest(30).index\n",
    "#     df['is_spam'] = df['url'].isin(spam_urls) \n",
    "    \n",
    "    #remove user/subreddit mentions\n",
    "    df['text'] = df['text'].str.replace(r\"(/r/\\w+)\", \" \", regex=True)\n",
    "    df['text'] = df['text'].str.replace(r\"(/u/\\w+)\", \" \", regex=True)\n",
    "    \n",
    "    #remove urls\n",
    "    df['text'] = df['text'].str.replace(r\"http[s]?://\\S+\", \" \", regex=True)\n",
    "    \n",
    "    #extra spaces\n",
    "    df['text'] = df['text'].str.replace(r\"\\s+\", \" \", regex=True)\\\n",
    "        .str.strip()          #leading/trailing spaces\n",
    "    \n",
    "    df['text'] = df['text'].fillna('').astype(str)\n",
    "    return df\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "972dcd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new df with sentences, authors instead of text, authors\n",
    "#runs on cleaned text\n",
    "def to_sentences(row):\n",
    "    author = row['author']\n",
    "    text = row['clean_text']\n",
    "    post_id = row.name\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        print(f\"Non-string value found: {text} (type: {type(text)})\")\n",
    "        return []\n",
    "    \n",
    "    all_sent = sent_tokenize(text)\n",
    "\n",
    "    df_sent = [{'author': author, 'sentence': sentence, 'post_id': post_id} for sentence in all_sent]\n",
    "    \n",
    "    return df_sent\n",
    "\n",
    "def text_cleaner(post):\n",
    "    \n",
    "    post = post.lower()  #lower case all text\n",
    "    \n",
    "    post = re.sub(r'[^a-zA-Z0-9\\s.!?]', '', post) #remove special char (not punctuation)\n",
    "    \n",
    "    post = re.sub(r\"(?<!\\.)\\n\", \". \", post)  # line breaks to sentence breaks\n",
    "    post = re.sub(r\"([a-zA-Z0-9])\\n\", r\"\\1. \", post)  # make sentences end properly\n",
    "    \n",
    "    #remove 0 width space character\n",
    "    post = re.sub('\\x200b', '', post)\n",
    "    \n",
    "    #quo thing\n",
    "    post = re.sub('quid pro quo', 'quidproquo', post)\n",
    "    post = re.sub('status quo', 'statusquo', post)\n",
    "    post = re.sub('dems', 'democrats', post)\n",
    "    \n",
    "    #name processing\n",
    "    post = re.sub(r\"(\\bgov|mr|ms|dr|jr)\\.\", r\"\\1\", post)\n",
    "    post = re.sub(r\"\\b(?:realdonaldtrump|donald j\\. trump)\", \"donald trump\", post)\n",
    "    \n",
    "    names_places = (\n",
    "        r'\\b('\n",
    "        r'donald trump|kamala harris|joe biden|hunter biden|bernie sanders|barack obama|michelle obama|'\n",
    "        r'elizabeth warren|mitch mcconnell|alex jones|nancy pelosi|betsy devos|robert mueller|ron desantis|'\n",
    "        r'ilhan omar|marjorie taylor greene|clarence thomas|kevin mccarthy|chris christie|pete buttigieg|'\n",
    "        r'el paso|new york|white house|new york city|new hampshire|north dakota|south dakota|'\n",
    "        r'north carolina|south carolina|rhode island|new mexico|new jersey|west virginia'\n",
    "        r')'\n",
    "        r'(?=\\b|[.!?]\\s|$)'\n",
    "    )\n",
    "    post = re.sub(names_places, lambda x: x.group().replace(\" \", \"\"), post)\n",
    "\n",
    "    #politicians more commonly known by 1 word \n",
    "    post = re.sub('alexandria ocasio cortez', 'aoc', post)\n",
    "    post = re.sub('beto orourke', 'beto', post)\n",
    "    \n",
    "    #post = re.sub('', '', post)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb4f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes stop words, and lemmatizes\n",
    "def lemma_er(post):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    post = [word for word in post if word not in stop_words]\n",
    "    \n",
    "    post = [lemmatizer.lemmatize(word) for word in post]\n",
    "    \n",
    "    return post\n",
    "\n",
    "#tag parts of speech (mostly for PPN ID)\n",
    "def pos_tagger(tokens):\n",
    "    doc = en_spacy(\" \".join(tokens))  \n",
    "    return [(token.text, token.pos_) for token in doc]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c249c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main functions to actually call\n",
    "#filter spam, preprocess, split text into sentences  \n",
    "def get_sentences(df):\n",
    "    data = concat(df)\n",
    "\n",
    "    data = filter_spam(data).copy()\n",
    "    \n",
    "    #clean text\n",
    "    data['clean_text'] = data['text'].apply(text_cleaner)\n",
    "\n",
    "    #split text into sentences - nec for fasttext\n",
    "    sentence_data = data.apply(to_sentences, axis=1)\n",
    "    flat_sentences = [item for sublist in sentence_data for item in sublist]\n",
    "    \n",
    "    return pd.DataFrame(flat_sentences)\n",
    "\n",
    "#returns sentences split into words and author\n",
    "def get_words_nltk(sent):\n",
    "    sents = sent.copy()\n",
    "    \n",
    "    #remove punctuation\n",
    "    sents['sentence'] = sents['sentence'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
    "\n",
    "    #tokenize words \n",
    "    sents['tokens'] = sents['sentence'].apply(lambda x: word_tokenize(x) if x.strip() else [])\n",
    "\n",
    "    #remove empty token rows\n",
    "    sents = sents[sents['tokens'].apply(lambda x: len(x) > 0)]\n",
    "    \n",
    "    #tag parts of speech\n",
    "    sents['pos_tags'] = sents['tokens'].apply(pos_tagger)\n",
    "    \n",
    "    #lemmatize\n",
    "    sents.loc[:, 'out_sent'] = sents['tokens'].apply(lemma_er)\n",
    "    \n",
    "    return sents\n",
    "\n",
    "#batch process sentences with spacy\n",
    "def get_words(sent):\n",
    "    sents = sent.copy()\n",
    "    \n",
    "    docs = list(en_spacy.pipe(sents['sentence'].tolist()))\n",
    "    \n",
    "    sents['tokens'] = [[token.text for token in doc] for doc in docs]\n",
    "    sents['pos_tags'] = [[(token.text, token.pos_) for token in doc] for doc in docs]\n",
    "    sents['out_sent'] = [[token.lemma_ for token in doc] for doc in docs]\n",
    "    \n",
    "    #remove empties\n",
    "    sents = sents[sents['tokens'].apply(len) > 0]\n",
    "    \n",
    "    return sents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9df2a",
   "metadata": {},
   "source": [
    "# RUN MAIN\n",
    "change to run on combo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4c84d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d19 = pd.read_csv('./data/raw/r_democrats_19down_auth.csv')\n",
    "data_d23 = pd.read_csv('./data/raw/r_democrats_23down_auth.csv')\n",
    "data_r23 = pd.read_csv('./data/raw/r_republican_23down_auth.csv')\n",
    "data_r19 = pd.read_csv('./data/raw/r_republican_19down_auth.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d895af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_r19 = get_sentences(data_r19)\n",
    "words_r19 = get_words(sents_r19)\n",
    "\n",
    "sents_r23 = get_sentences(data_r23)\n",
    "words_r23 = get_words(sents_r23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08146597",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_r23.to_csv('./data/lemma/r_republican_23_words.csv', index = False)\n",
    "words_r19.to_csv('./data/lemma/r_republican_19_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d5100c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_d23 = get_sentences(data_d23)\n",
    "words_d23 = get_words(sents_d23)\n",
    "\n",
    "sents_d19 = get_sentences(data_d19)\n",
    "words_d19 = get_words(sents_d19)\n",
    "\n",
    "words_d23.to_csv('./data/lemma/r_democrats_23_words.csv', index = False)\n",
    "words_d19.to_csv('./data/lemma/r_democrats_19_words.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1a0d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts that became empty after cleaning: 133\n",
      "Posts that resulted in no sentences: 138\n"
     ]
    }
   ],
   "source": [
    "#check empties\n",
    "\n",
    "# data_filtered = filter_spam(concat((data_r23)))\n",
    "\n",
    "# data_filtered['clean_text'] = data_filtered['text'].apply(text_cleaner)\n",
    "# print(\"Posts that became empty after cleaning:\", (data_filtered['clean_text'] == \"\").sum())\n",
    "\n",
    "# empty_sentences = data_filtered['clean_text'].apply(lambda x: len(sent_tokenize(x)) == 0).sum()\n",
    "# print(\"Posts that resulted in no sentences:\", empty_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649d79d",
   "metadata": {},
   "source": [
    "# POS TAG\n",
    "dictionary\n",
    "will do analysis on NNP(S) separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7634f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_POS(df):\n",
    "    #creat dictionary\n",
    "    pos_dict = defaultdict(set)\n",
    "\n",
    "    for pos_list in df['pos_tags']:\n",
    "        for word, tag in pos_list:\n",
    "            pos_dict[word].add(tag)\n",
    "\n",
    "    #convert to df\n",
    "    df_word_pos = pd.DataFrame(list(pos_dict.items()), columns=['word', 'tags'])\n",
    "    \n",
    "    return df_word_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fa23a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_r19 = get_POS(words_r19)\n",
    "pos_r19.to_csv('./data/lemma/r_republican_19_pos.csv', index = False)\n",
    "\n",
    "pos_d19 = get_POS(words_d19)\n",
    "pos_d19.to_csv('./data/lemma/r_democrats_19_pos.csv', index = False)\n",
    "\n",
    "pos_r23 = get_POS(words_r23)\n",
    "pos_r23.to_csv('./data/lemma/r_republican_23_pos.csv', index = False)\n",
    "\n",
    "pos_d23 = get_POS(words_d23)\n",
    "pos_d23.to_csv('./data/lemma/r_democrats_23_pos.csv', index = False)\n",
    "#SAVE DICTS FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36840a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29188 30428 27460 29575\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_d23), len(pos_d19), len(pos_r23), len(pos_r19))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549241fa",
   "metadata": {},
   "source": [
    "# TTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e334c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTR for D23: 0.038347237732378636\n",
      "TTR for D19: 0.03244635815541579\n",
      "TTR for R23: 0.035267705775375795\n",
      "TTR for R19: 0.033832363071875614\n"
     ]
    }
   ],
   "source": [
    "#compute token-text ratio for corpus\n",
    "#input: words_ df\n",
    "def ttr(df):\n",
    "    #flatten\n",
    "    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "    unique_tok = set(all_tokens)\n",
    "\n",
    "    return len(unique_tok) / len(all_tokens)\n",
    "\n",
    "#compute for each corpus\n",
    "print(\"TTR for D23:\", ttr(words_d23))\n",
    "print(\"TTR for D19:\", ttr(words_d19))\n",
    "print(\"TTR for R23:\", ttr(words_r23))\n",
    "print(\"TTR for R19:\", ttr(words_r19))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
